Language Models are Super Mario:
Absorbing Abilities from Homologous Models as a Free Lunch
Le Yu 1 Bowen Yu 1 Haiyang Yu 1 Fei Huang 1 Yongbin Li 1
Abstract
In this paper, we unveil that Language Models
(LMs) can acquire new capabilities by assimilat-
ing parameters from homologous models without
retraining or GPUs. We first introduce DARE to
set most delta parameters (i.e., the disparity be-
tween fine-tuned and pre-trained parameters) to
zeros without affecting the abilities of Supervised
Fine-Tuning (SFT) LMs, which randomly Drops
delta parameters with a ratio p And REscales the
remaining ones by 1/(1 −p) to approximate the
original embeddings. Then, we use DARE as a
versatile plug-and-play technique to sparsify delta
parameters of multiple SFT homologous models
for mitigating parameter interference and merge
them into a single model by parameter fusing.
We experiment with encoder- and decoder-based
LMs, showing that: (1) SFT delta parameter value
ranges are typically small (within 0.005) with
extreme redundancy, and DARE can effortlessly
eliminate 90% or even 99% of them. (2) DARE
can merge multiple task-specific LMs into one
LM with diverse capabilities. For instance, the
amalgamation of WizardLM and WizardMath sig-
nificantly enhances the GSM8K zero-shot accu-
racy of WizardLM from 2.2 to 66.3, retaining the
instruction-following proficiency while surpass-
ing WizardMath’s 64.2 performance. Our merged
LM also ranks first among models with 7 billion
parameters on the Open LLM Leaderboard.
1. Introduction
Human beings have harbored a longstanding desire to ac-
quire additional abilities through various ways, as expressed
in mediums like movies and games. For example, in X-
Men’s Apocalypse, the character can absorb the powers
of other mutants to strengthen himself. Likewise, the pro-
tagonist in the Super Mario games can gain superpowers
1Alibaba
Group.
Correspondence
to:
Bowen
Yu
<yubowen.ybw@alibaba-inc.com>,
Yongbin
Li
<shuide.lyb@alibaba-inc.com>.
Figure 1: (Left) DARE can effectively eliminate 90% or
even 99% delta parameters of WizardMath on GSM8K.
(Right) DARE can merge multiple task-specific SFT lan-
guage models into a single model with all the abilities.
like throwing fireballs by absorbing in-game items. In this
paper, we astonishingly find that Language Models (LMs),
similar to Apocalypse and Super Mario, can enhance their
capabilities by absorbing other models without the need for
retraining or even GPUs.
Formally, Supervised Fine-Tuning (SFT) is the most widely
adopted strategy for unlocking task-specific abilities to LMs
by optimizing their parameters (Dodge et al., 2020; Zhao
et al., 2023). The effectiveness of SFT is fully evident
in the alteration of the model parameters before and after
SFT, referred to as delta parameters (Ding et al., 2023).
We first show that SFT LM (either encoder- or decoder-
based) always tends to acquire excessively redundant delta
parameters. To be specific, we present DARE (Drop And
REscale), which randomly sets certain delta parameters
to zeros with a drop rate p and subsequently rescales the
remaining ones by a factor 1/(1−p). Although conceptually
simple, DARE can eliminate up to 99% delta parameters
with minimal impact on the performance when the LM’s
parameters reach 70 billion (see Figure 1(a)). Moreover,
the more parameters the LM has, the larger p it can tolerate.
We attribute the effectiveness of DARE to its ability to
approximate the original embeddings, which is verified both
theoretically and empirically.
Furthermore, we can merge multiple homologous SFT LMs
(fine-tuned from the same backbone) based on DARE with-
out compromising their capabilities. As long as a small
portion of the delta parameters remain unaffected during
merging, the abilities of LMs unlocked by SFT can still be
1
arXiv:2311.03099v2  [cs.CL]  4 Feb 2024
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
preserved. We first employ DARE to eliminate redundant
delta parameters in each model before merging, which can
potentially mitigate the interference of parameters among
multiple models (Yadav et al., 2023). Then, we apply estab-
lished model merging techniques (Wortsman et al., 2022;
Ilharco et al., 2023; Matena & Raffel, 2022; Jin et al., 2023;
Yadav et al., 2023) to fuse the parameters with reduced re-
dundancy for creating one model with diverse capabilities.
We conduct extensive experiments with encoder-based LMs
on GLUE benchmark, and decoder-based LMs with three
distinct abilities: instruction-following, mathematical rea-
soning, and code-generating. We observe that:
(1) SFT LMs exhibit a substantial number of redundant
delta parameters regardless of their backbones (e.g., BERT,
RoBERTa, LLaMA, Llama 2, or Code Llama). DARE can
remove 90% or even 99% delta parameters without signifi-
cantly affecting the model performance. DARE is able to
approximate the original embeddings well and provide very
similar embeddings for each layer of the LM. The rescale
operation is crucial to guarantee the success of DARE, and
dropping 30% or 40% delta parameters without rescaling
would noticeably lead to worse results.
(2) DARE can often enhance the performance of various
model merging methods on encoder-based LMs. For larger
decoder-based LMs, simply averaging the parameters can
already yield surprisingly good results. As shown in Figure
1(b), we merge WizardLM and WizardMath by combining
DARE and parameter averaging, leading to a significant im-
provement of WizardLM’s mathematical reasoning ability
with zero-shot accuracy from 2.2 to 66.3 on GSM8K, while
also modestly enhancing its instruction-following ability
with win rate from 67.2 to 67.5 on AlpacaEval. We also
offer a merged LM with 7 billion parameters and it attains
the top-ranking position on the Open LLM Leaderboard. It
is fascinating that all these benefits are achieved by solely
using CPUs without retraining.
(3) SFT delta parameters usually stay within 0.005, indi-
cating minimal modifications to the pre-trained LM, and
DARE works for delta parameters with relatively small
value ranges. However, once models undergo continuous
pre-training, the delta parameters can rapidly reach around
0.03, making DARE infeasible. Moreover, dropping only
10% fine-tuned parameters (i.e., the combination of pre-
trained and delta parameters) would lead to a catastrophic
decrease in performance, even approaching zero. This find-
ing further confirms that SFT primarily unlocks the abilities
of pre-trained LMs, rather than introducing new capabilities.
The used resources are publicly available at https:
//github.com/yule-BUAA/MergeLM, which inte-
grates existing popular model merging methods and sup-
ports both encoder- and decoder-based LMs.
2. Related Work
Supervised Fine-tuning of Language Models. SFT of
LMs aims to impart pre-trained LMs with particular abilities
by optimizing them on task-specific data, which has become
the de facto standard paradigm in natural language process-
ing (Dodge et al., 2020; Zhao et al., 2023). Generally, SFT
can be divided into two categories: full fine-tuning (Radford
et al., 2018; Devlin et al., 2019) and parameter-efficient fine-
tuning (Houlsby et al., 2019; Liu et al., 2021; Li & Liang,
2021; Lester et al., 2021; Hu et al., 2022). Indeed, the effects
of SFT are reflected by the difference between parameters
of LMs before and after SFT, i.e., delta parameters. In this
paper, we reveal the extreme redundancy of various SFT
LMs’ delta parameters by proposing an innovative approach
DARE, achieving competitive performance with standard
SFT LMs by removing 90% or even 99% delta parameters.
Network Pruning Technique. With the rapidly increasing
size of neural networks, network pruning technique has been
widely applied to reduce the computational costs (Cheng
et al., 2017; Liang et al., 2021). The objective of network
pruning is to eliminate unnecessary parameters while main-
taining the model performance (Zhu & Gupta, 2018; Liu
et al., 2019b; Frankle & Carbin, 2019; Gale et al., 2019;
Xia et al., 2022). Magnitude-based pruning is one classi-
cal pruning method, which selects parameters according
to their magnitudes (i.e., absolute parameter values) (Han
et al., 2015; Li et al., 2018; Lee et al., 2021). To be specific,
parameters with magnitudes lower than a certain threshold
are removed, and others are preserved. In fact, DARE is
relevant to the concept of network pruning as it can also
drop parameters. But DARE differs from existing pruning
techniques in: (1) DARE focuses on delta parameters while
most pruning methods deal with fine-tuned parameters; (2)
DARE can work well without any retraining or extra data,
which are often inevitably required by pruning methods.
Model Merging. Model merging has become a trending
research direction in recent years, aiming to merge multi-
ple task-specific models into a single model with diverse
abilities (Wortsman et al., 2022; Matena & Raffel, 2022;
Ilharco et al., 2023; Jin et al., 2023; Yadav et al., 2023;
Zhang et al., 2023). The superiority of model merging over
multi-task learning (Crawshaw, 2020; Zhang & Yang, 2022)
(which also intends to obtain one model with several abili-
ties) is that model merging pays attention to the fusion of
model parameters without accessing the original training
data (Matena & Raffel, 2022; Jin et al., 2023). Average
Merging (Wortsman et al., 2022) is one common model
merging approach, which utilizes averaged parameters to
construct the merged model. Task Arithmetic (Ilharco et al.,
2023) employs a pre-defined scaling term to distinguish the
importance of various models. Fisher Merging (Matena
& Raffel, 2022) performs weighted fusions of parameters,
2
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
where the weights are calculated by the Fisher information
matrix (Fisher, 1922). RegMean (Jin et al., 2023) masterly
solves model merging by optimizing a linear regression
problem with closed-form solutions. TIES-Merging (Ya-
dav et al., 2023) tackles the task conflicts in Ilharco et al.
(2023) by trimming low-magnitude parameters, resolving
sign disagreements, and disjointly merging parameters with
consistent signs. In this paper, we use DARE as a versa-
tile plug-in for existing model merging methods by first
sparsifying delta parameters of several SFT homologous
models and then merging them into a single model, which
is equipped with the capabilities of all the SFT models.
3. Methodology
SFT Delta Parameters. Let θPRE ∈Rd denote the param-
eters of a pre-trained LM (d is the parameter dimension),
such as LLaMA (Touvron et al., 2023a) or Llama 2 (Touvron
et al., 2023b). For task t, SFT can provide a fine-tuned LM
with parameters θt
SFT ∈Rd by optimizing the pre-trained
model on task-specific data. Give the parameters of both
pre-trained LM (θPRE) and SFT LM (θt
SFT), delta parameters
are defined as the difference between parameters of LMs
before and after SFT, i.e., δt = θt
SFT −θPRE ∈Rd. Since
delta parameters reflect the changes in parameters during the
SFT process, analyzing the properties of delta parameters
can offer a better understanding of SFT.
Model Merging Problem.
Given a set of K tasks
{t1, t2, · · · , tK} and K corresponding SFT models with
parameters

θt1
SFT, θt2
SFT, · · · , θtK
SFT
	
, model merging aims to
fuse the parameters of K models into a single model that
can well handle K tasks simultaneously. Following Matena
& Raffel (2022); Jin et al. (2023); Yadav et al. (2023), we
focus on merging fine-tuned models that are optimized from
the same pre-trained backbone.
3.1. DARE: A Simple Approach for Reducing Delta
Parameter Redundancy
In this work, we reveal the extremely redundant properties
of the delta parameters of SFT LMs and propose DARE to
effectively reduce delta parameter redundancy (see Figure
2(a)). DARE is conceptually simple and consists of two
steps: drop and rescale. Given delta parameters δt = θt
SFT−
θPRE, DARE first performs random drop on δt based on a
drop rate p (setting their values to zeros) and then rescales
the remaining ones by a factor 1/(1 −p) as follows,
mt ∼Bernoulli(p),
e
δt =
1 −mt
⊙δt,
(1)
ˆ
δt = e
δt/(1 −p).
Finally, we combine ˆ
δt and θPRE via addition to obtain the
parameters for inference, i.e., θt
DARE = ˆ
δt+θPRE. We prove
that even after removing most delta parameters, DARE can
well preserve the model performance by approximating the
original embeddings.
Theoretical Analysis. We discuss linear transformation
since most parameters of LMs play a role in this basic
operation (e.g., the computations in feed-forward networks,
the projections of queries, keys, values, and outputs in self-
attention modules). Let W /∆W ∈Rm×n and b/∆b ∈
Rm be the pre-trained/delta parameters. The input is a vector
x ∈Rn. Expectation of the i-th (1 ≤i ≤m) dimension of
the original embeddings h ∈Rm is computed by
E[hi] = E[
n
X
j=1
(wij + ∆wij) xj + (bi + ∆bi)]
=
n
X
j=1
xjE[wij] + E[bi] +
n
X
j=1
xjE[∆wij] + E[∆bi]
=
n
X
j=1
wijxj + bi +
n
X
j=1
∆wijxj + ∆bi = hPRE
i
+ ∆hi
Assuming that DARE randomly drops delta parameters with
a ratio p, and rescales others by a factor γ. With DARE, the
delta parameters change to ∆c
W ∈Rm×n and ∆b
b ∈Rm.
Expectation of the i-th dimension of embeddings becomes
E[ˆ
hi] = E[
n
X
j=1
(wij + ∆ˆ
wij) xj + (bi + ∆ˆ
bi)]
=
n
X
j=1
xjE[wij] + E[bi] +
n
X
j=1
xjE[∆ˆ
wij] + E[∆ˆ
bi]
=
n
X
j=1
wijxj + bi +
n
X
j=1
xj((1 −p) · γ · ∆wij + p · 0)
+((1 −p) · γ · ∆bi + p · 0)
= hPRE
i
+ (1 −p) · γ · (
n
X
j=1
∆wijxj + ∆bi)
= hPRE
i
+ (1 −p) · γ · ∆hi
By setting γ = 1/(1 −p), we have E[hi] = E[ˆ
hi], conclud-
ing that DARE can approximate the original embeddings.
Remark. We have given a rough proof of why DARE works.
In practice, we find that DARE is applicable when the drop
rate p is properly set, and the tolerance of p grows with LMs’
parameter sizes. Moreover, removing fine-tuned rather than
delta parameters would cause a catastrophically decreased
performance. A promising future direction is to explore
DARE more deeply, such as inferring the upper bound of p
with respect to LM capacities and illustrating the intrinsic
difference between fine-tuned and delta parameters.
3
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
Figure 2: Illustrations of DARE and merging models with DARE. DARE can achieve comparable performance with standard
SFT with 90% or even 99% delta parameters removed. Moreover, DARE tackles the parameter interference issue when
merging models and yields consistent improvements. We denote the abilities of icons for math/code-related tasks at the top.
Last, we highlight the connections and differences between
DARE and Dropout (Srivastava et al., 2014). Both methods
involve random dropping and rescaling operations, but they
differ in two key aspects: (1) DARE handles delta parame-
ters while Dropout operates on model outputs; (2) DARE
aims to reduce delta parameter redundancy without training,
which permanently eliminates delta parameters and only re-
tains others for inference. Dropout is used to prevent models
from overfitting, which temporarily removes part of outputs
during training but preserves all the outputs for inference.
3.2. Merging Models with DARE
As DARE effectively reduces the redundancy of delta pa-
rameters by setting most of them to zeros, we hypothesize
that DARE can help address the interference of parameters
when merging multiple models (Yadav et al., 2023). Take
Figure 2(b) as an example, when merging math- and code-
related models, DARE can assist existing model merging
methods to better absorb the abilities of two models with
less or no parameter interference.
Formally, given K models that are fine-tuned on K corre-
sponding tasks with parameters

θt1
SFT, θt2
SFT, · · · , θtK
SFT
	
, we
first apply DARE on each parameters θtk
SFT (1 ≤k ≤K),
and derive

θt1
DARE, θt2
DARE, · · · , θtK
DARE
	
. Then, we adopt
established model merging methods to fuse the derived pa-
rameters and obtain the merged single model. Let us take
Task Arithmetic (Ilharco et al., 2023) as an instance, whose
official computation process is denoted by
θM = θPRE+λ·
K
X
k=1
δtk = θPRE+λ·
K
X
k=1
(θtk
SFT−θPRE), (2)
where λ is the scaling term to determine the importance of
the models to be merged. When equipped with DARE, the
calculation process of Task Arithmetic is rewritten as
θtk
DARE = DARE
 θtk
SFT, θPRE

, for 1 ≤k ≤K,
θM = θPRE + λ ·
K
X
k=1
ˆ
δtk = θPRE + λ ·
K
X
k=1
(θtk
DARE −θPRE).
(3)
In Section 4.3, we find that DARE can effectively improve
the performance of Task Arithmetic when merging multiple
LMs. It is also worth noticing that DARE is a versatile plug-
and-play module and can be applied to any model merging
methods, such as Average Merging (Wortsman et al., 2022),
Fisher Merging (Matena & Raffel, 2022), RegMean (Jin
et al., 2023), and TIES-Merging (Yadav et al., 2023).
4
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
4. Experiments
4.1. Experimental Setup
Datasets and Pre-Trained Backbones for Decoder-based
LMs. We choose AlpacaEval (Li et al., 2023) for evaluating
instruction-following models (WizardLM (Xu et al., 2023)).
We use GSM8K (Cobbe et al., 2021) and MATH (Hendrycks
et al., 2021b) for testing mathematical reasoning models
(WizardMath (Luo et al., 2023a)). HumanEval (Chen et al.,
2021) and MBPP (Austin et al., 2021) are adopted for esti-
mating code-generating models (WizardCoder-Python (Luo
et al., 2023b) and llama-2-13b-code-alpaca (Chaudhary,
2023)). These models are fine-tuned based on pre-trained
backbones including LLaMA (Touvron et al., 2023a), Llama
2 (Touvron et al., 2023b), and Code Llama (Rozi`
ere et al.,
2023). Please see Table 3 in Section A.1 for their versions
and correspondences with pre-trained backbones.
Datasets and Pre-Trained Backbones for Encoder-based
LMs. For encoder-based LMs, the GLUE benchmark (Wang
et al., 2019) is used, containing one sentence acceptabil-
ity dataset CoLA (Warstadt et al., 2019), one sentiment
detection dataset SST-2 (Socher et al., 2013), two para-
phrase datasets MRPC (Dolan & Brockett, 2005) and QQP
(Shankar et al., 2017), one sentence similarity dataset STS-
B (Cer et al., 2017), and three natural language inference
datasets MNLI (Bowman et al., 2015; Williams et al., 2018),
QNLI (Rajpurkar et al., 2016), and RTE (Dagan et al., 2005;
Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al.,
2009). As the test labels of GLUE are not publicly available,
we split the original training data into training and validation
sets with ratios of 90% and 10%. The original validation
data is used as the test set. We choose bert-base-uncased
(Devlin et al., 2019) and roberta-base (Liu et al., 2019a)
as pre-trained backbones, and further fine-tune them to get
SFT models on the eight datasets.
Evaluation Metrics. We calculate win rate for AlpacaEval,
zero-shot accuracy for GSM8K and MATH, pass@1 for
HumanEval and MBPP, Matthews correlation coefficient
for CoLA, accuracy for SST-2, QNLI, and RTE, matched
accuracy for MNLI, accuracy and F1 score for MRPC and
QQP, and Pearson and Spearman correlation for STS-B.
Implementation Details. Following Xu et al. (2023); Luo
et al. (2023a;b), the inference of decoder-based LMs is
implemented by vLLM (Kwon et al., 2023). Temperature
is set to 0.0 for greedy decoding. The maximal number of
generated tokens is 1,024 on GSM8K, and 2,048 on the
other four datasets. For encoder-based LMs, We fine-tune
bert-base-uncased and roberta-base for 10 epochs with a
warmup strategy. The weight decay is 0.01. We use 1e-5
and 5e-5 as learning rates and list the optimal setting of each
fine-tuned model in Table 4 in Section A.2. Experiments
are conducted on NVIDIA Tesla V100 and A100 GPUs.
4.2. Extreme Redundancy in SFT Delta Parameters
We show the extremely redundant property of SFT delta
parameters of both decoder- and encoder-based LMs. We
vary drop rate p in [0.0, 0.1, 0.2, · · · , 0.9, 0.99] and apply
DARE to get models after removing the corresponding ratio
of delta parameters. When p is equal to 0.0, we actually
obtain the standard SFT LMs. We report the performance
of decoder-based LMs on GSM8K and HumanEval as well
as encoder-based LMs on eight GLUE datasets in Figure 3
and Figure 4. Please see results of decoder-based LMs on
AlpacaEval, MATH, and MBPP in Figure 12 in Section B.1.
Figure 3: Performance of decoder-based LMs on GSM8K
and HumanEval with various drop rates.
Figure 4: Performance of encoder-based LMs on GLUE
with different drop rates.
We conclude that: (1) the SFT delta parameters of both
encoder- and decoder-based LMs are highly redundant.
DARE can effectively remove 90% delta parameters without
significantly decreasing the performance. In some cases,
the drop rate p can even reach 99%; (2) the tolerance of
drop rate increases with the sizes of LMs, i.e., LMs with
more parameters can withstand higher drop rate. For exam-
ple, WizardMath-70B performs well when p = 0.99 while
WizardMath-7B and WizardMath-13B fail. This depicts
some connections with the scaling laws of LMs (Kaplan
et al., 2020; Hoffmann et al., 2022), indicating that there
may exist quantifiable correlations between model sizes and
drop rates they can afford.
4.3. Merging Models with DARE on SFT LMs
We combine DARE with five model merging methods, in-
cluding Average Merging (Wortsman et al., 2022), Task
5
Arithmetic (Ilharco et al., 2023), Fisher Merging (Matena
& Raffel, 2022), RegMean (Jin et al., 2023), and TIES-
Merging (Yadav et al., 2023). Please see Section A.3 for
more descriptions. For feasible computations, we evalu-
ate decoder-based LMs with Task Arithmetic by choosing
the scaling term in [0.5, 1.0] and report the best results.
We merge WizardLM-13B, WizardMath-13B, and llama-
2-13b-code-alpaca since all of them adopt Llama-2-13b as
the pre-trained backbone. WizardCoder-Python-13B is not
selected as it is fine-tuned from CodeLlama-13b-Python.
We merge encoder-based LMs with all five methods and
perform grid search on some hyperparameters (see Table 5
in Section A.4 for more details). Following Jin et al. (2023);
Yadav et al. (2023), we also fine-tune the models under
the multi-task learning setting and report the oracle results.
We show partial results of merging decoder-based LMs and
encoder-based LMs in Table 1 and Figure 5. Please refer to
Table 6 and Figure 13 in Section B.2 for more results.
Table 1: Performance of merging decoder-based WizardLM-
13B (LM), WizardMath-13B (Math), and llama-2-13b-code-
alpaca (Code). We use single and mixed colors to denote
individual and merged models. The best and second-best
results are marked in bold and underlined fonts.
Merging
Methods
Models
Use
DARE
AlpacaEval
GSM8K
MBPP
Single
Model
LM
No
67.20
2.20
34.00
Math
No
/
64.22
/
Code
No
/
/
27.60
Task
Arithmetic
LM
No
67.04
66.34
30.60
& Math
Yes
67.45
66.26
32.40
LM
No
68.07
/
32.40
& Code
Yes
67.83
/
33.00
Math
No
/
64.67
8.60
& Code
Yes
/
65.05
9.80
LM & Math
No
69.03
58.45
29.80
& Code
Yes
69.28
56.48
31.60
In Table 1, DARE often facilitates Task Arithmetic on
merging decoder-based LMs, yielding better results than
the single model in many cases. For instance, merging
WizardLM-13B and WizardMath-13B effectively improves
WizardLM-13B’s zero-shot accuracy from 2.2 to 66.3 on
GSM8K (better than WizardMath-13B’s 64.2 performance),
and also enhances the instruction-following ability with win
rate from 67.2 to 67.5 on AlpacaEval. Moreover, since
llama-2-13b-code-alpaca is not well fine-tuned for gener-
ating codes (it performs worse than WizardLM-13B), we
hypothesize this will affect the model merging performance.
Hence, we additionally evaluate the code-generating abil-
ity of the merger of WizardLM-13B and WizardMath-13B,
which obtains better results than llama-2-13b-code-alpaca,
explaining the suboptimal performance of the amalgamation
of WizardMath-13B and llama-2-13b-code-alpaca. There-
fore, a potential prerequisite for effective model merging is
that each model to be merged should be well fine-tuned.
Figure 5: Performance of merging encoder-based bert-base-
uncased and roberta-base on CoLA and MRPC.
From Figure 5, we observe that DARE often yields modestly
better results of various merging methods, achieving an av-
erage improvement of 0.58%, 0.36%, 0.37%, -0.03%, and
0.84% on Average Merging, Task Arithmetic, Fisher Merg-
ing, RegMean, and TIES Merging. However, the merged
model still struggles to surpass the single model in some
cases, which is in line with the conclusion in Matena &
Raffel (2022); Jin et al. (2023); Yadav et al. (2023).
Table 2: Results of 7B LMs on the Open LLM Leaderboard.
Models
Average ARC Hella. MMLU TQA Wino. GSM8K
NeuralBeagle14-7B
74.74
72.95 88.34
64.55 69.93 82.40
70.28
Beagle14-7B
74.76
72.95 87.95
64.70 68.88 82.64
71.42
supermario v1
74.85
73.72 88.71
64.57 68.23 85.64
68.23
WildMarcoroni-7B
75.29
73.98 88.61
64.81 69.76 84.29
70.28
WestSeverus-7B
75.29
71.42 88.27
64.79 72.37 83.27
71.65
supermario v2
75.49
72.95 88.53
64.99 71.22 83.90
71.34
We further provide two merged LMs with 7 billion parame-
ters (supermario v1 and supermario v2) and evaluate them
on Open LLM Leaderboard (Beeching et al., 2023). Please
see Section A.5 for more details. From Table 2, we find
that the merged LMs beat the individual models they are
built upon, achieving considerable improvements. Notably,
until January 28th, 2024, supermario v2 achieves the first
rank on the Open LLM Leaderboard. It is exciting that these
benefits are cheaply obtained by only using CPUs.
4.4. Importance of the Rescale Operation
As analyzed in Section 3.1, the rescale operation in DARE
is essential to approximate the original embeddings. To
verify this, we introduce DropOnly which randomly drops
delta parameters without rescaling. We calculate the similar-
ities of embeddings between the original LM and LM with
DARE or DropOnly. Specifically, we obtain the embeddings
of each input token layer-by-layer and report the average
cosine similarities. Results of WizardMath-7B on GSM8K
and bert-base-uncased on CoLA are shown in Figure 6. We
observe that DARE can perfectly maintain the original em-
beddings in each layer with similarities higher than 0.95
even when removing 90% delta parameters. However, Dro-
6
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
Figure 6: Cosine similarities of each layer’s embeddings
between the original LM and LM with DARE or DropOnly.
pOnly just preserves the original embeddings with p = 0.1
and the similarities sharply decline when p is higher. For
example, the similarities on WizardMath-7B decrease to
about 0.85/0.68 when p is 0.5/0.9). We further show the
distributions of embeddings’ cosine similarities in the last
layer in Figure 7, demonstrating the ability of DARE in ap-
proximating original embeddings. Note that similar findings
can be obtained on other LMs and datasets but they are not
presented due to page limits.
Figure 7: Distributions of cosine similarities of the last
layer’s embeddings between the original LM and LM with
DARE or DropOnly.
We also report the performance of LMs with DARE and
DropOnly in Figure 8. See Figure 14 and Figure 15 in Sec-
tion B.3 for additional results. We observe that discarding
the rescale operation usually leads to worse results, and the
performance gaps between DARE and DropOnly become
more significant with the increase of p. This validates the
effectiveness of the rescale operation in DARE once again.
4.5. Comparison with Magnitude-based Pruning
We compare DARE with the commonly used Magnitude-
based Pruning (MP) (Han et al., 2015; Li et al., 2018; Lee
et al., 2021), which chooses parameters based on their mag-
nitudes. For more fair and credible comparisons, we adapt
MP to operate on delta parameters and discard the retraining
process. We show partial results of LMs with DARE and
Figure 8: Comparisons between DARE and DropOnly on
GSM8K and CoLA on various LMs.
MP in Figure 9. Please refer to Figure 16 and Figure 17
in Section B.4 for extra results. We find that DARE out-
performs MP in most cases and the superiority of DARE is
more obvious when the drop rate becomes higher, verifying
the superiority of DARE in abandoning delta parameters.
We have also tried to combine MP with the rescale opera-
tion but got worse results than using MP separately. This is
because MP removes parameters with smaller magnitudes
and retains certain parameters with the largest magnitudes.
Simply rescaling the remaining ones would destroy the orig-
inal embeddings and result in unpredictable performance.
Figure 9: Comparisons between DARE and MP on GSM8K
and CoLA on various LMs.
4.6. When Can DARE Be Used?
We investigate the prerequisites that DARE can work. We
choose Llama-2-13b instead of CodeLlama-13b-Python as
the pre-trained backbone for WizardCoder-Python-13B and
apply DARE to derive the model after dropping certain delta
parameters for evaluation. We find that the pass@1 metric
on HumanEval/MBPP drastically decreases from 63.41/55.4
to 0.0/0.0 when only 10% delta parameters are removed.
We deduce this is because Code Llama models are addition-
ally trained with 500B tokens of code-related data (Rozi`
ere
et al., 2023), resulting in more obvious changes in parameter
7
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
Figure 10: Delta parameter absolute values of 13B decoder-
based LMs vs. the pre-trained backbones.
values with respect to Llama 2 models. Since WizardCoder-
Python-13B is fine-tuned based on CodeLlama-13b-Python,
when it uses Llama-2-13b as the pre-trained backbone, the
ranges of SFT delta parameters would become much larger,
making DARE infeasible. To verify this, we depict the ab-
solute values of SFT delta parameters of 13B decoder-based
LMs vs. various pre-trained backbones in Figure 10. Please
see Figure 18, Figure 19 and Figure 20 in Section B.5 for the
SFT delta parameter ranges on decoder- and encoder-based
LMs. Note that the results are plotted by randomly choosing
10% delta parameters due to the huge parameter numbers
of LMs. We also present the statistics about the deciles of
delta parameter ranges of both decoder- and encoder-based
LMs in Table 7 in Section B.5.
From the results, we observe the absolute values of delta
parameters of WizardCoder-Python-13B vs. Llama-2-13b
(often greater than 0.01) are several orders of magnitude big-
ger than those of WizardCoder-Python-13B vs. CodeLlama-
13b-Python (usually within 0.0002), causing the failure of
DARE. For other 13B decoder-based LMs fine-tuned from
Llama-2-13b, most of their absolute values of delta parame-
ters are less than 0.005, making DARE a proper choice. To
this end, we conclude that DARE can work well when the
absolute values of SFT delta parameters are relatively small
(e.g., less than 0.005). Otherwise, DARE may fail.
4.7. Can DARE Drop Fine-tuned Parameters?
As previous network pruning methods mainly operate on
the fine-tuned instead of delta parameters, we also conduct
experiments under this setting. For decoder-based LMs,
we find they perform badly when removing fine-tuned pa-
rameters even with 0.1 as the drop rate. Quantitatively, the
performance sharply drops from 67.20 to 8.56 on AlpacaE-
val for WizardLM-13B, from 64.22/14.02 to 0.38/0.16 on
GSM8K/MATH for WizardMath-13B, from 63.41/55.40 to
0.0/0.20 on HumanEval/MBPP for WizardCoder-Python-
13B. Similar observations can also be found on MP or
decoder-based LMs with 7B, 34B, or 70B sizes. Partial
results on encoder-based LMs are shown in Figure 11 and
Figure 11: Results of DARE and MP by dropping fine-tuned
parameters on CoLA and MRPC on encoder-based LMs.
please see Figure 21 in Section B.6 for additional results. We
observe that directly eliminating the fine-tuned parameters
by either DARE or MP would lead to worse performance
on encoder-based LMs. This confirms that the knowledge
is inherent in pre-trained LMs, and SFT is responsible for
unlocking instead of introducing new capabilities.
5. Conclusion
In this work, we first discussed the extremely redundant
properties of SFT delta parameters in LMs and proposed a
simple approach DARE to effectively reduce the number of
delta parameters needed for SFT without any data, retrain-
ing, or even GPUs. DARE can impressively drop 90% or
even 99% SFT delta parameters without sacrificing much
performance compared with using all SFT delta parameters.
We further employed DARE as a versatile plug-and-play
approach for existing model merging methods to merge mul-
tiple task-specific fine-tuned models into a single model
with diverse abilities. Extensive experimental results on
both encoder- and decoder-based LMs demonstrated the
effectiveness of DARE in reducing SFT delta parameter
redundancy and facilitating the model merging performance.
We also provided a deeper analysis of why DARE works as
well as the prerequisites for using DARE.
Impact Statements
Recently, merging language models has become a promising
research direction. Our work allows researchers to obtain a
single model with diverse capabilities at a low cost. Thanks
to our method, hundreds of models with different functional-
ities and domains have been created on the Hugging Face
community. Several popular toolkits for model merging
have also been established on the GitHub platform. Even
though this work has no direct social impacts, the poten-
tially harmful information generated by LLMs (e.g., gender
bias, racial discrimination) may still exist when using our
approach. It is necessary to advocate for careful regulation
by communities and governments on this matter.
8
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
References
Austin, J., Odena, A., Nye, M. I., Bosma, M., Michalewski,
H., Dohan, D., Jiang, E., Cai, C. J., Terry, M., Le, Q. V.,
and Sutton, C. Program synthesis with large language
models. CoRR, abs/2108.07732, 2021.
Beeching, E., Fourrier, C., Habib, N., Han, S., Lambert,
N., Rajani, N., Sanseviero, O., Tunstall, L., and Wolf, T.
Open llm leaderboard, 2023.
Bentivogli, L., Clark, P., Dagan, I., and Giampiccolo, D.
The fifth pascal recognizing textual entailment challenge.
TAC, 7:8, 2009.
Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D.
A large annotated corpus for learning natural language
inference. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing, pp.
632–642. The Association for Computational Linguistics,
2015.
Cer, D. M., Diab, M. T., Agirre, E., Lopez-Gazpio, I., and
Specia, L. Semeval-2017 task 1: Semantic textual simi-
larity - multilingual and cross-lingual focused evaluation.
CoRR, abs/1708.00055, 2017.
Chaudhary, S. Code alpaca: An instruction-following llama
model for code generation. https://github.com/
sahil280114/codealpaca, 2023.
Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto,
H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N.,
Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov,
M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray,
S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar-
ian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D.,
Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A.,
Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang,
J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W.,
Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra,
V., Morikawa, E., Radford, A., Knight, M., Brundage,
M., Murati, M., Mayer, K., Welinder, P., McGrew, B.,
Amodei, D., McCandlish, S., Sutskever, I., and Zaremba,
W. Evaluating large language models trained on code.
CoRR, abs/2107.03374, 2021.
Cheng, Y., Wang, D., Zhou, P., and Zhang, T. A survey
of model compression and acceleration for deep neural
networks. CoRR, abs/1710.09282, 2017.
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,
Schoenick, C., and Tafjord, O. Think you have solved
question answering? try arc, the AI2 reasoning challenge.
CoRR, abs/1803.05457, 2018.
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,
Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,
R., Hesse, C., and Schulman, J. Training verifiers to solve
math word problems. CoRR, abs/2110.14168, 2021.
Crawshaw, M. Multi-task learning with deep neural net-
works: A survey. CoRR, abs/2009.09796, 2020.
Dagan, I., Glickman, O., and Magnini, B. The PASCAL
recognising textual entailment challenge. In Candela,
J. Q., Dagan, I., Magnini, B., and d’Alch´
e-Buc, F. (eds.),
Machine Learning Challenges, Evaluating Predictive Un-
certainty, Visual Object Classification and Recognizing
Textual Entailment, First PASCAL Machine Learning
Challenges Workshop, volume 3944 of Lecture Notes
in Computer Science, pp. 177–190. Springer, 2005.
Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:
pre-training of deep bidirectional transformers for lan-
guage understanding. In Proceedings of the 2019 Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Human Language Tech-
nologies, pp. 4171–4186. Association for Computational
Linguistics, 2019.
Ding, N., Qin, Y., Yang, G., Wei, F., Yang, Z., Su, Y., Hu, S.,
Chen, Y., Chan, C., Chen, W., Yi, J., Zhao, W., Wang, X.,
Liu, Z., Zheng, H., Chen, J., Liu, Y., Tang, J., Li, J., and
Sun, M. Parameter-efficient fine-tuning of large-scale pre-
trained language models. Nat. Mac. Intell., 5(3):220–235,
2023.
Dodge, J., Ilharco, G., Schwartz, R., Farhadi, A., Hajishirzi,
H., and Smith, N. A. Fine-tuning pretrained language
models: Weight initializations, data orders, and early
stopping. CoRR, abs/2002.06305, 2020.
Dolan, W. B. and Brockett, C. Automatically construct-
ing a corpus of sentential paraphrases. In Proceedings
of the Third International Workshop on Paraphrasing,
IWP@IJCNLP 2005. Asian Federation of Natural Lan-
guage Processing, 2005.
Fisher, R. A. On the mathematical foundations of theoretical
statistics. Philosophical transactions of the Royal Society
of London. Series A, containing papers of a mathematical
or physical character, 222(594-604):309–368, 1922.
Frankle, J. and Carbin, M. The lottery ticket hypothesis:
Finding sparse, trainable neural networks. In 7th Interna-
tional Conference on Learning Representations. OpenRe-
view.net, 2019.
Gale, T., Elsen, E., and Hooker, S. The state of sparsity in
deep neural networks. CoRR, abs/1902.09574, 2019.
Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,
A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H.,
McDonell, K., Muennighoff, N., Ociepa, C., Phang, J.,
Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L.,
9
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A
framework for few-shot language model evaluation, 12
2023.
Giampiccolo, D., Magnini, B., Dagan, I., and Dolan, W. B.
The third pascal recognizing textual entailment challenge.
In Proceedings of the ACL-PASCAL workshop on textual
entailment and paraphrasing, pp. 1–9, 2007.
Haim, R. B., Dagan, I., Dolan, B., Ferro, L., Giampiccolo,
D., Magnini, B., and Szpektor, I. The second pascal recog-
nising textual entailment challenge. In Proceedings of the
Second PASCAL Challenges Workshop on Recognising
Textual Entailment, volume 7, pp. 785–794, 2006.
Han, S., Pool, J., Tran, J., and Dally, W. Learning both
weights and connections for efficient neural network.
Advances in neural information processing systems, 28,
2015.
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,
Song, D., and Steinhardt, J. Measuring massive multitask
language understanding. In 9th International Conference
on Learning Representations. OpenReview.net, 2021a.
Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,
S., Tang, E., Song, D., and Steinhardt, J. Measuring
mathematical problem solving with the MATH dataset.
In Proceedings of the Neural Information Processing
Systems Track on Datasets and Benchmarks 1, 2021b.
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,
Cai, T., Rutherford, E., de Las Casas, D., Hendricks,
L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E.,
Millican, K., van den Driessche, G., Damoc, B., Guy,
A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W.,
Vinyals, O., and Sifre, L. Training compute-optimal large
language models. CoRR, abs/2203.15556, 2022.
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,
de Laroussilhe, Q., Gesmundo, A., Attariyan, M., and
Gelly, S. Parameter-efficient transfer learning for NLP.
In Proceedings of the 36th International Conference on
Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 2790–2799. PMLR, 2019.
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
S., Wang, L., and Chen, W. Lora: Low-rank adaptation of
large language models. In The Tenth International Con-
ference on Learning Representations. OpenReview.net,
2022.
Ilharco, G., Ribeiro, M. T., Wortsman, M., Schmidt, L.,
Hajishirzi, H., and Farhadi, A. Editing models with task
arithmetic. In The Eleventh International Conference on
Learning Representations. OpenReview.net, 2023.
Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
Chaplot, D. S., de Las Casas, D., Bressand, F., Lengyel,
G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.,
Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T.,
and Sayed, W. E. Mistral 7b. CoRR, abs/2310.06825,
2023.
Jin, X., Ren, X., Preotiuc-Pietro, D., and Cheng, P. Data-
less knowledge fusion by merging weights of language
models. In The Eleventh International Conference on
Learning Representations. OpenReview.net, 2023.
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,
Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and
Amodei, D. Scaling laws for neural language models.
CoRR, abs/2001.08361, 2020.
Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,
C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient
memory management for large language model serving
with pagedattention. In Proceedings of the 29th Sym-
posium on Operating Systems Principles, pp. 611–626.
ACM, 2023.
Lee, J., Park, S., Mo, S., Ahn, S., and Shin, J. Layer-
adaptive sparsity for the magnitude-based pruning. In 9th
International Conference on Learning Representations.
OpenReview.net, 2021.
Lester, B., Al-Rfou, R., and Constant, N. The power of scale
for parameter-efficient prompt tuning. In Proceedings of
the 2021 Conference on Empirical Methods in Natural
Language Processing, pp. 3045–3059. Association for
Computational Linguistics, 2021.
Li, G., Qian, C., Jiang, C., Lu, X., and Tang, K. Optimiza-
tion based layer-wise magnitude-based pruning for DNN
compression. In Proceedings of the Twenty-Seventh Inter-
national Joint Conference on Artificial Intelligence, pp.
2383–2389. ijcai.org, 2018.
Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I.,
Guestrin, C., Liang, P., and Hashimoto, T. B. Alpacae-
val: An automatic evaluator of instruction-following
models.
https://github.com/tatsu-lab/
alpaca_eval, 2023.
Li, X. L. and Liang, P. Prefix-tuning: Optimizing continu-
ous prompts for generation. In Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference
on Natural Language Processing, pp. 4582–4597. Asso-
ciation for Computational Linguistics, 2021.
Liang, T., Glossner, J., Wang, L., Shi, S., and Zhang, X.
Pruning and quantization for deep neural network accel-
eration: A survey. Neurocomputing, 461:370–403, 2021.
10
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
Lin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring
how models mimic human falsehoods. In Proceedings
of the 60th Annual Meeting of the Association for Com-
putational Linguistics, pp. 3214–3252. Association for
Computational Linguistics, 2022.
Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., and
Tang, J. GPT understands, too. CoRR, abs/2103.10385,
2021.
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy,
O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta:
A robustly optimized BERT pretraining approach. CoRR,
abs/1907.11692, 2019a.
Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re-
thinking the value of network pruning. In 7th Interna-
tional Conference on Learning Representations. OpenRe-
view.net, 2019b.
Luo, H., Sun, Q., Xu, C., Zhao, P., Lou, J., Tao, C., Geng, X.,
Lin, Q., Chen, S., and Zhang, D. Wizardmath: Empower-
ing mathematical reasoning for large language models via
reinforced evol-instruct. CoRR, abs/2308.09583, 2023a.
Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C.,
Ma, J., Lin, Q., and Jiang, D. Wizardcoder: Empowering
code large language models with evol-instruct. CoRR,
abs/2306.08568, 2023b.
Matena, M. and Raffel, C. Merging models with fisher-
weighted averaging. In Advances in Neural Information
Processing Systems, 2022.
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.,
et al. Improving language understanding by generative
pre-training. 2018.
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad:
100, 000+ questions for machine comprehension of text.
In Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing, pp. 2383–2392.
The Association for Computational Linguistics, 2016.
Rozi`
ere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan,
X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov,
A., Evtimov, I., Bitton, J., Bhatt, M., Canton-Ferrer, C.,
Grattafiori, A., Xiong, W., D´
efossez, A., Copet, J., Azhar,
F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and
Synnaeve, G. Code llama: Open foundation models for
code. CoRR, abs/2308.12950, 2023.
Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.
Winogrande: An adversarial winograd schema challenge
at scale. In The Thirty-Fourth AAAI Conference on Artifi-
cial Intelligence, pp. 8732–8740. AAAI Press, 2020.
Shankar, I., Nikhil, D., and Kornel, C.
First quora
dataset release: question pairs (2017). URL https://www.
quora. com/q/quoradata/First-Quora-Dataset-Release-
Question-Pairs, 2017.
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,
C. D., Ng, A. Y., and Potts, C. Recursive deep models
for semantic compositionality over a sentiment treebank.
In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pp. 1631–1642.
ACL, 2013.
Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I.,
and Salakhutdinov, R. Dropout: a simple way to prevent
neural networks from overfitting. J. Mach. Learn. Res.,
15(1):1929–1958, 2014.
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
M., Lacroix, T., Rozi`
ere, B., Goyal, N., Hambro, E.,
Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-
ple, G. Llama: Open and efficient foundation language
models. CoRR, abs/2302.13971, 2023a.
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
Bhosale, S., Bikel, D., Blecher, L., Canton-Ferrer, C.,
Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu,
J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N.,
Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas,
M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A.,
Koura, P. S., Lachaux, M., Lavril, T., Lee, J., Liskovich,
D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra,
P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J.,
Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith,
E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor,
R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I.,
Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez,
A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2:
Open foundation and fine-tuned chat models.
CoRR,
abs/2307.09288, 2023b.
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and
Bowman, S. R. GLUE: A multi-task benchmark and anal-
ysis platform for natural language understanding. In 7th
International Conference on Learning Representations.
OpenReview.net, 2019.
Warstadt, A., Singh, A., and Bowman, S. R. Neural net-
work acceptability judgments. Trans. Assoc. Comput.
Linguistics, 7:625–641, 2019.
Williams, A., Nangia, N., and Bowman, S. R. A broad-
coverage challenge corpus for sentence understanding
through inference. In Walker, M. A., Ji, H., and Stent, A.
(eds.), Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational
11
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
Linguistics: Human Language Technologies, pp. 1112–
1122. Association for Computational Linguistics, 2018.
Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Lopes,
R. G., Morcos, A. S., Namkoong, H., Farhadi, A., Car-
mon, Y., Kornblith, S., and Schmidt, L. Model soups: av-
eraging weights of multiple fine-tuned models improves
accuracy without increasing inference time. In Interna-
tional Conference on Machine Learning, volume 162 of
Proceedings of Machine Learning Research, pp. 23965–
23998. PMLR, 2022.
Xia, M., Zhong, Z., and Chen, D. Structured pruning learns
compact and accurate models. In Muresan, S., Nakov,
P., and Villavicencio, A. (eds.), Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 1513–1528.
Association for Computational Linguistics, 2022.
Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J.,
Tao, C., and Jiang, D. Wizardlm: Empowering large
language models to follow complex instructions. CoRR,
abs/2304.12244, 2023.
Yadav, P., Tam, D., Choshen, L., Raffel, C., and Bansal,
M. Resolving interference when merging models. In Ad-
vances in Neural Information Processing Systems, 2023.
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.
Hellaswag: Can a machine really finish your sentence? In
Proceedings of the 57th Conference of the Association for
Computational Linguistics, pp. 4791–4800. Association
for Computational Linguistics, 2019.
Zhang, J., Chen, S., Liu, J., and He, J. Composing parameter-
efficient modules with arithmetic operations. In Advances
in Neural Information Processing Systems, 2023.
Zhang, Y. and Yang, Q. A survey on multi-task learning.
IEEE Trans. Knowl. Data Eng., 34(12):5586–5609, 2022.
Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y.,
Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C.,
Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X.,
Liu, Z., Liu, P., Nie, J., and Wen, J. A survey of large
language models. CoRR, abs/2303.18223, 2023.
Zhu, M. and Gupta, S. To prune, or not to prune: Exploring
the efficacy of pruning for model compression. In 6th
International Conference on Learning Representations.
OpenReview.net, 2018.
12
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
A. Detailed Experimental Settings
A.1. Details of SFT and Pre-Trained Backbones of Decoder-based LMs
Table 3 shows the versions and correspondences with pre-trained backbones of SFT decoder-based LMs.
Table 3: Versions and correspondences with pre-trained backbones of SFT decoder-based LMs.
Tasks
SFT Decoder-based LMs
Pre-Trained Backbones
Instruction-following
WizardLM-7B1
llama-7b2
WizardLM-13B3
Llama-2-13b4
WizardLM-70B5
Llama-2-70b6
Mathematical Reasoning
WizardMath-7B7
Llama-2-7b8
WizardMath-13B9
Llama-2-13b4
WizardMath-70B10
Llama-2-70b6
Code-generating
WizardCoder-Python-7B11
CodeLlama-7b-Python12
WizardCoder-Python-13B13
CodeLlama-13b-Python14
WizardCoder-Python-34B15
CodeLlama-34b-Python16
llama-2-13b-code-alpaca17
Llama-2-13b4
A.2. Learning Rate Configurations of Encoder-based LMs on GLUE
The optimal settings of the learning rate of each fine-tuned encoder-based LM are presented in Table 4.
Table 4: Configurations of learning rates of bert-base-uncased and roberta-base on GLUE.
Models
CoLA
SST-2
MRPC
STS-B
QQP
MNLI
QNLI
RTE
bert-base-uncased18
5e-5
1e-5
5e-5
5e-5
1e-5
1e-5
1e-5
1e-5
roberta-base19
1e-5
1e-5
5e-5
1e-5
1e-5
1e-5
1e-5
1e-5
A.3. Descriptions of Existing Model Merging Methods
We experiment with five model merging methods:
• Average Merging simply averages the parameters of multiple models to get the merged model (Wortsman et al., 2022).
1https://huggingface.co/WizardLM/WizardLM-7B-V1.0
2https://huggingface.co/decapoda-research/llama-7b-hf
3https://huggingface.co/WizardLM/WizardLM-13B-V1.2
4https://huggingface.co/meta-llama/Llama-2-13b-hf
5https://huggingface.co/WizardLM/WizardLM-70B-V1.0
6https://huggingface.co/meta-llama/Llama-2-70b-hf
7https://huggingface.co/WizardLM/WizardMath-7B-V1.0
8https://huggingface.co/meta-llama/Llama-2-7b-hf
9https://huggingface.co/WizardLM/WizardMath-13B-V1.0
10https://huggingface.co/WizardLM/WizardMath-70B-V1.0
11https://huggingface.co/WizardLM/WizardCoder-Python-7B-V1.0
12https://huggingface.co/codellama/CodeLlama-7b-Python-hf
13https://huggingface.co/WizardLM/WizardCoder-Python-13B-V1.0
14https://huggingface.co/codellama/CodeLlama-13b-Python-hf
15https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0
16https://huggingface.co/codellama/CodeLlama-34b-Python-hf
17https://huggingface.co/layoric/llama-2-13b-code-alpaca
18https://huggingface.co/bert-base-uncased
19https://huggingface.co/roberta-base
13
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
• Task Arithmetic uses a scaling term to control the contributions between the pre-trained backbone and the models to
be merged (Ilharco et al., 2023).
• Fisher Merging first estimates the importance of parameters by calculating the Fisher information matrix, and then
fuses parameters based on their importance (Matena & Raffel, 2022).
• RegMean recasts the model merging task as a linear regression problem and derives closed-form solutions to solve the
problem (Jin et al., 2023).
• TIES-Merging aims to address parameter conflicts in model merging. It first trims parameters with lower magnitudes,
and then resolves sign disagreements. Parameters with consistent signs are finally merged (Yadav et al., 2023).
A.4. Details of Grid Search on Hyperparameters of Model Merging Methods for Encoder-based LMs
Table 5 shows the searched ranges of model merging methods’ hyperparameters for encoder-based LMs. For DARE, we
search the drop rate p in [0.1, 0.2, · · · , 0.9] and select the optimal setting with the best performance.
Table 5: Searched ranges of hyperparameters of model merging methods for encoder-based LMs.
Model Merging Methods
Search Ranges of Hyperparameters
Task Arithmetic
scaling term to merge model parameters: [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
Fisher Merging
scaling term to merge model parameters: [0.1, 0.3, 0.5, 0.7, 0.9, 1.0],
number of examples to compute Fisher information matrix: [256, 512, 1024, 2048]
RegMean
scaling term to reduce non-diagonal items: [0.1, 0.3, 0.5, 0.7, 0.9, 1.0],
number of examples to compute inner product matrices: [256, 512, 1024, 2048]
TIES-Merging
scaling term to merge model parameters: [0.1, 0.3, 0.5, 0.7, 0.9, 1.0],
ratio to retain parameters with largest-magnitude values: [0.1, 0.2, 0.3]
A.5. Details of Our Merged 7B LMs and the Open LLM Leaderboard
We offer two merged LMs with 7 billion parameters, namely supermario v1 and supermario v2. Specifically, we choose
NeuralBeagle14-7B20 and Turdus21 to build supermario v1, where both of them all derived from Beagle14-7B22. We set the
drop rate p in DARE to 0.3, and merge NeuralBeagle14-7B and Turdus by Task Arithmetic with 0.8 as the scaling term. We
select WildMarcoroni-Variant1-7B23 and WestSeverus-7B-DPO-v224 to obtain supermario v2, where both of them adopt
Mistral-7B-v0.125 (Jiang et al., 2023) as the backbone. The drop rate p in DARE is set to 0.5, and the scaling term in Task
Arithmetic is also 0.5.
The Open LLM Leaderboard26 is established to evaluate open-sourced LLMs based on Eleuther AI Language Model
Evaluation Harness (Gao et al., 2023), which contains six benchmarks including AI2 Reasoning Challenge (ARC) (Clark
et al., 2018), HellaSwag(Zellers et al., 2019), MMLU(Hendrycks et al., 2021a), TruthfulQA(Lin et al., 2022), Wino-
grande(Sakaguchi et al., 2020), and GSM8K(Cobbe et al., 2021). The average score on the six datasets is used for ranking
models on the leaderboard. We refer interested readers to the original papers for detailed information on the datasets.
Note that due to space limits, in Table 2, we use Hella., TQA and Wino. as abbreviations for HellaSwag, TruthfulQA,
and Winogrande. WildMarcoroni-7B and WestSeverus-7B are the abbreviations for WildMarcoroni-Variant1-7B and
WestSeverus-7B-DPO-v2.
20https://huggingface.co/mlabonne/NeuralBeagle14-7B
21https://huggingface.co/udkai/Turdus
22https://huggingface.co/mlabonne/Beagle14-7B
23https://huggingface.co/BarryFutureman/WildMarcoroni-Variant1-7B
24https://huggingface.co/FelixChao/WestSeverus-7B-DPO-v2
25https://huggingface.co/mistralai/Mistral-7B-v0.1
26https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
14
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
B. Additional Experimental Results
B.1. Additional Results of Delta Parameter Redundancy of Decoder-based LMs
Figure 12 shows results of decoder-based LMs on AlpacaEval, MATH, and MBPP with different drop rates. We notice
that the performance of WizardLM-70B drastically declines on AlpacaEval when the drop rate is 0.9 (different from the
observations of WizardMath-70B and WizardCoder-Python-34B). One possible reason is that the instruction-following
task on AlpacaEval is harder and requires general abilities with more delta parameters via SFT, causing more obvious
dependencies among parameters (especially on LMs with larger sizes). Therefore, when the ratio of dropped delta parameters
reaches a relatively small value (e.g., 0.9 in this case), the dependent relationships among parameters are destroyed, leading
to unsatisfactory performance.
Figure 12: Performance of decoder-based LMs on AlpacaEval, MATH, and MBPP with various drop rates.
B.2. Additional Results of Merging LMs with DARE
Table 6 presents the complete performance of merging decoder-based LMs. Figure 13 shows the performance of merging
encoder-based LMs on GLUE.
Table 6: Performance of merging decoder-based WizardLM-13B (LM), WizardMath-13B (Math), and llama-2-13b-code-
alpaca (Code) on all the datasets. We use blue, green, and red colors to distinguish each single model and utilize mixed
colors to denote the merged model. The best and second-best results are marked in bold and underlined fonts.
Merging
Methods
Models
Use
DARE
Instruction-
following
Mathematical
Reasoning
Code-
generating
AlpacaEval
GSM8K
MATH
HumanEval
MBPP
Single
Model
LM
No
67.20
2.20
0.04
36.59
34.00
Math
No
/
64.22
14.02
/
/
Code
No
/
/
/
23.78
27.60
Task
Arithmetic
LM
No
67.04
66.34
13.40
28.66
30.60
& Math
Yes
67.45
66.26
12.86
26.83
32.40
LM
No
68.07
/
/
31.70
32.40
& Code
Yes
67.83
/
/
35.98
33.00
Math
No
/
64.67
13.98
8.54
8.60
& Code
Yes
/
65.05
13.96
10.37
9.80
LM & Math
No
69.03
58.45
9.88
18.29
29.80
& Code
Yes
69.28
56.48
10.16
23.17
31.60
B.3. Additional Results of Comparisons between DARE and DropOnly
The comparison results between DARE and DropOnly on AlpacaEval, MATH, HumanEval, and MBPP on decoder-based
LMs and all results on GLUE on encoder-based LMs are shown in Figure 14 and Figure 15, respectively.
15
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
Figure 13: Performance of merging encoder-based LMs on GLUE.
Figure 14: Comparing DARE and DropOnly on AlpacaEval, MATH, HumanEval, and MBPP on decoder-based LMs.
Figure 15: Comparisons between DARE and DropOnly on GLUE on encoder-based LMs.
16
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
B.4. Additional Results of Comparisons between DARE and MP
Comparisons between DARE and magnitude-based pruning on AlpacaEval, MATH, HumanEval, and MBPP on decoder-
based LMs and all results on GLUE on encoder-based LMs are shown in Figure 16 and Figure 17, respectively.
Figure 16: Comparisons between DARE and MP on AlpacaEval, MATH, HumanEval, and MBPP on decoder-based LMs.
B.5. Ranges of SFT Delta Parameters of Decoder-based LMs and Encoder-based LMs
We show the SFT delta parameter ranges of decoder- and encoder-based LMs in Figure 18, Figure 19 and Figure 20. We
give the statistics about the deciles of delta parameter ranges in Table 7, which are computed on 0.1%/10% randomly chosen
parameters of decoder-based/encoder-based LMs for feasibility.
Figure 17: Comparisons between DARE and MP on GLUE on encoder-based LMs.
B.6. Additional Results of Dropping Fine-tuned Parameters on Encoder-based LMs
Figure 21 shows the results of removing fine-tuned parameters on GLUE on encoder-based LMs.
17
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
Figure 18: Delta parameter ranges of 13B decoder-based LMs vs. the pre-trained backbones.
Figure 19: Delta parameter ranges of bert-base-uncased after SFT on GLUE.
18
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
Figure 20: Delta parameter ranges of roberta-base after SFT on GLUE.
Figure 21: Performance of DARE and MP when dropping fine-tuned parameters on GLUE on encoder-based LMs.
19
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
Table 7: Statistics about the deciles of delta parameter ranges of both decoder- and encoder-based LMs.
Models
min
10%
20%
30%
40%
50%
60%
70%
80%
90%
max
WizardLM-13B
vs. Llama-2-13b
-0.0285
-0.0016
-0.0010
-0.0006
-0.0003
0.0000
0.0003
0.0006
0.0010
0.0016
0.0341
WizardMath-13B
vs. Llama-2-13b
-0.0048
-0.0006
-0.0004
-0.0002
-0.0001
0.0000
0.0001
0.0002
0.0004
0.0006
0.0047
llama-2-13b-code-alpaca
vs. Llama-2-13b
-7.3242e-04 -3.0518e-05 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 3.0518e-05 7.3242e-04
WizardCoder-Python-13B
vs. CodeLlama-13b-Python -1.3733e-03 -1.8120e-04 -1.0681e-04 -6.8665e-05 -3.4332e-05 0.0000e+00 3.3379e-05 6.8665e-05 1.0681e-04 1.8120e-04 1.4648e-03
WizardCoder-Python-13B
vs. Llama-2-13b
-0.5192
-0.0381
-0.0247
-0.0153
-0.0073
0.0000
0.0073
0.0153
0.0247
0.0381
0.7246
bert-base-uncased
CoLA
-1.0062e-02 -1.9900e-03 -1.1620e-03 -5.6932e-04 -7.3511e-05 3.0810e-05 1.0663e-04 5.7345e-04 1.1661e-03 1.9936e-03 1.0592e-02
SST-2
-7.3227e-03 -1.3287e-03 -8.0843e-04 -4.2815e-04 -1.1123e-04 4.8140e-05 1.9256e-04 4.4204e-04 8.2212e-04 1.3412e-03 6.6411e-03
MRPC
-8.0786e-03 -1.4064e-03 -8.4427e-04 -4.4834e-04 -1.0060e-04 8.5477e-06 1.0558e-04 4.5363e-04 8.4961e-04 1.4107e-03 9.7023e-03
STS-B
-1.3253e-02 -1.8758e-03 -1.1282e-03 -5.9211e-04 -1.2640e-04 2.1029e-05 1.2679e-04 5.9258e-04 1.1304e-03 1.8794e-03 9.5631e-03
QQP
-0.0216
-0.0032
-0.0019
-0.0011
-0.0004
0.0002
0.0006
0.0013
0.0021
0.0032
0.0306
MNLI
-1.9360e-02 -2.6150e-03 -1.6359e-03 -9.5239e-04 -3.9054e-04 7.4751e-05 4.7965e-04 1.0297e-03 1.6948e-03 2.6518e-03 2.2719e-02
QNLI
-1.2412e-02 -1.7661e-03 -1.1141e-03 -6.5727e-04 -2.7595e-04 4.4333e-05 3.3008e-04 7.0713e-04 1.1566e-03 1.7944e-03 1.0278e-02
RTE
-4.2236e-03 -3.9333e-04 -2.4141e-04 -1.3099e-04 -3.4146e-05 2.1234e-06 3.5670e-05 1.3276e-04 2.4325e-04 3.9541e-04 4.3249e-03
roberta-base
CoLA
-3.2903e-03 -4.9453e-04 -2.6499e-04 -1.0061e-04 -2.5153e-05 1.8091e-06 2.8946e-05 1.0061e-04 2.6651e-04 4.9653e-04 3.0104e-03
SST-2
-7.5466e-03 -1.1795e-03 -6.6938e-04 -3.4303e-04 -1.4345e-04 1.0615e-05 1.6433e-04 3.8091e-04 6.8196e-04 1.1915e-03 7.6943e-03
MRPC
-7.0281e-03 -1.2577e-03 -6.8674e-04 -2.9566e-04 -5.8427e-05 2.8922e-06 7.0035e-05 2.9370e-04 6.8584e-04 1.2580e-03 8.0592e-03
STS-B
-2.6313e-03 -4.4548e-04 -2.5395e-04 -1.1136e-04 -2.1163e-05 1.2186e-06 3.3885e-05 1.1186e-04 2.5456e-04 4.4597e-04 2.3694e-03
QQP
-2.8996e-02 -3.1613e-03 -2.0129e-03 -1.1793e-03 -5.3198e-04 6.3781e-05 6.5139e-04 1.3161e-03 2.1409e-03 3.2998e-03 2.2995e-02
MNLI
-2.8375e-02 -3.3906e-03 -2.1666e-03 -1.2824e-03 -5.7767e-04 6.9943e-05 7.0991e-04 1.4290e-03 2.3107e-03 3.5429e-03 2.0889e-02
QNLI
-7.6891e-03 -1.2210e-03 -7.5125e-04 -4.3479e-04 -1.8336e-04 8.6576e-06 2.0984e-04 4.5467e-04 7.6704e-04 1.2338e-03 6.8666e-03
RTE
-1.6544e-03 -2.9030e-04 -1.6271e-04 -7.3047e-05 -1.0118e-05 3.8603e-07 1.2353e-05 7.3839e-05 1.6338e-04 2.9109e-04 1.7324e-03
20
